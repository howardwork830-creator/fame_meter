{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Celebrity Popularity Quantifier - Sentiment Pipeline v4.0\n",
    "\n",
    "**Taiwan Edition**\n",
    "\n",
    "This notebook processes social media posts for Taiwan celebrities, runs sentiment analysis using a Chinese BERT model, and generates popularity rankings.\n",
    "\n",
    "**Runtime Requirements:**\n",
    "- Python 3.10\n",
    "- GPU: P100 (free tier)\n",
    "- Estimated runtime: ~20 minutes\n",
    "\n",
    "**Kaggle Secrets Required:**\n",
    "- `GCP_JSON`: Google Cloud Service Account JSON credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 1: IMPORTS & SETUP\nimport pandas as pd\nimport numpy as np\nimport json\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ML imports\nfrom transformers import pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Google Sheets imports (using google-auth instead of oauth2client)\nimport gspread\nfrom google.oauth2.service_account import Credentials\n\n# Kaggle secrets\nfrom kaggle_secrets import UserSecretsClient\n\nprint(\"âœ“ All imports successful\")\nprint(f\"Run started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 2: AUTHENTICATE GOOGLE SHEETS\nimport re\nprint(\"Authenticating to Google Sheets...\")\n\n# Get credentials from Kaggle Secrets\nuser_secrets = UserSecretsClient()\ncreds_json = user_secrets.get_secret(\"GCP_JSON\")\ncreds_dict = json.loads(creds_json)\n\n# Fix private key formatting issues (common when pasting into web forms)\nif 'private_key' in creds_dict:\n    pk = creds_dict['private_key']\n    # Replace escaped newlines with actual newlines\n    pk = pk.replace('\\\\n', '\\n')\n    # Fix any extra spaces in BEGIN/END tags\n    pk = re.sub(r'-----BEGIN\\s+PRIVATE\\s+KEY-----', '-----BEGIN PRIVATE KEY-----', pk)\n    pk = re.sub(r'-----END\\s+PRIVATE\\s+KEY-----', '-----END PRIVATE KEY-----', pk)\n    # Remove any extra whitespace around the key\n    pk = pk.strip()\n    creds_dict['private_key'] = pk\n\n# Define scopes\nscopes = [\n    'https://www.googleapis.com/auth/spreadsheets',\n    'https://www.googleapis.com/auth/drive'\n]\n\n# Authenticate using google-auth\ncredentials = Credentials.from_service_account_info(creds_dict, scopes=scopes)\nclient = gspread.authorize(credentials)\n\nprint(\"âœ“ Authenticated to Google Sheets\")\nprint(f\"Service Account: {creds_dict.get('client_email', 'Unknown')}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 3: LOAD CONFIGURATION\n# Use Sheet ID directly (more reliable than name)\nSHEET_ID = \"1sgKkhqP0_WAzdBfBbH2oWLAav-WlGkbCyayLguaHG6Q\"\n\nprint(f\"Loading configuration from Sheet ID: {SHEET_ID}...\")\n\ntry:\n    spreadsheet = client.open_by_key(SHEET_ID)\n    print(f\"âœ“ Opened spreadsheet: {spreadsheet.title}\")\n    \n    config_sheet = spreadsheet.worksheet(\"Config\")\n    config_data = config_sheet.get_all_records()\n    \n    CONFIG = {}\n    for row in config_data:\n        key = row.get(\"Setting_Name\", \"\")\n        value = row.get(\"Value\", \"\")\n        \n        if not key:\n            continue\n            \n        if key == \"CELEBRITIES_TO_TRACK\":\n            CONFIG[key] = [s.strip() for s in str(value).split(\",\") if s.strip()]\n        elif \"WEIGHT\" in key or \"DAYS\" in key or \"MIN\" in key:\n            CONFIG[key] = int(value) if value else 0\n        elif \"THRESHOLD\" in key or \"MAX\" in key:\n            CONFIG[key] = float(value) if value else 0.0\n        else:\n            CONFIG[key] = value\n    \n    print(f\"âœ“ Loaded config: {len(CONFIG)} settings\")\n    print(f\"  Celebrities: {CONFIG.get('CELEBRITIES_TO_TRACK', [])}\")\n    \nexcept Exception as e:\n    print(f\"âš ï¸ Could not load Config sheet: {e}\")\n    print(\"Using default configuration...\")\n    CONFIG = {\n        'CELEBRITIES_TO_TRACK': ['è”¡ä¾æ—', 'ç‹å¿ƒå‡Œ', 'æŸ¯éœ‡æ±', 'æ—ä¿Šå‚‘', 'äº”æœˆå¤©'],\n        'MODEL_ACCURACY_THRESHOLD': 0.85,\n        'CONFIDENCE_THRESHOLD': 0.70,\n        'SENTIMENT_STDDEV_MAX': 0.25,\n        'TRAINING_DATA_MIN': 200\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 4: READ ALL DATA FROM RAW DATA SHEET\nprint(\"Loading all posts from Raw Data sheet...\")\n\ntry:\n    raw_sheet = spreadsheet.worksheet(\"Raw Data\")\n    raw_data = raw_sheet.get_all_records()\n    df_raw = pd.DataFrame(raw_data)\n    \n    print(f\"âœ“ Loaded {len(df_raw)} total posts\")\n    \n    if len(df_raw) > 0:\n        print(f\"\\nData summary:\")\n        print(f\"  Columns: {list(df_raw.columns)}\")\n        if 'Collection_Timestamp' in df_raw.columns:\n            print(f\"  Date range: {df_raw['Collection_Timestamp'].min()} to {df_raw['Collection_Timestamp'].max()}\")\n        if 'Celebrity' in df_raw.columns:\n            print(f\"\\nPosts by celebrity:\")\n            print(df_raw['Celebrity'].value_counts())\n        if 'Platform' in df_raw.columns:\n            print(f\"\\nPosts by platform:\")\n            print(df_raw['Platform'].value_counts())\n    else:\n        print(\"âš ï¸ No data found in Raw Data sheet - this is expected for first run\")\n        # Create empty DataFrame with expected columns\n        df_raw = pd.DataFrame(columns=[\n            'Collection_Timestamp', 'Celebrity', 'Platform', 'Account_Name',\n            'Post_Content', 'Post_URL', 'Post_Timestamp',\n            'Account_Type', 'Feedback', 'Feedback_Notes', 'Sentiment_Score', 'Processing_Date'\n        ])\nexcept Exception as e:\n    print(f\"âš ï¸ Error loading Raw Data: {e}\")\n    print(\"Creating empty DataFrame...\")\n    df_raw = pd.DataFrame(columns=[\n        'Collection_Timestamp', 'Celebrity', 'Platform', 'Account_Name',\n        'Post_Content', 'Post_URL', 'Post_Timestamp',\n        'Account_Type', 'Feedback', 'Feedback_Notes', 'Sentiment_Score', 'Processing_Date'\n    ])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 5: LOAD SENTIMENT MODEL\n# CRITICAL FIX 1.8: Proper GPU detection instead of bare except\nimport torch\n\nMODEL_NAME = \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\"\n\nprint(f\"Loading sentiment model: {MODEL_NAME}...\")\nprint(\"This may take a few minutes on first run...\")\n\n# Proper device detection\nif torch.cuda.is_available():\n    device = 0\n    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\nelse:\n    device = -1  # CPU\n    print(\"âš ï¸ No GPU detected, using CPU (slower)\")\n\ntry:\n    sentiment_pipeline = pipeline(\n        \"sentiment-analysis\",\n        model=MODEL_NAME,\n        device=device\n    )\n    print(f\"âœ“ Model loaded successfully ({'GPU' if device == 0 else 'CPU'})\")\nexcept Exception as e:\n    print(f\"ERROR loading model: {e}\")\n    raise RuntimeError(f\"Failed to load sentiment model: {e}\")\n\n# Test the model with timeout protection\ntry:\n    test_result = sentiment_pipeline(\"é€™æ˜¯ä¸€å€‹å¾ˆæ£’çš„ç”¢å“ï¼\")[0]\n    print(f\"\\nModel test: 'é€™æ˜¯ä¸€å€‹å¾ˆæ£’çš„ç”¢å“ï¼'\")\n    print(f\"  Label: {test_result['label']}, Score: {test_result['score']:.4f}\")\nexcept Exception as e:\n    print(f\"ERROR: Model test failed: {e}\")\n    raise RuntimeError(f\"Model inference failed: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 6: SENTIMENT ANALYSIS ON ALL POSTS\n# CRITICAL FIX 1.11: Track errors instead of silently mapping to 0\nprint(\"Running sentiment analysis on all posts...\")\nprint(f\"Processing {len(df_raw)} posts...\")\n\nsentiments = []\nerrors = []\n\nfor idx, row in df_raw.iterrows():\n    # Get post content, truncate to 512 chars (model limit)\n    text = str(row.get('Post_Content', '')).strip()\n    \n    # Handle empty/NaN content explicitly\n    if pd.isna(text) or text == '' or text.lower() == 'nan':\n        sentiments.append(None)  # Use None instead of 0 to indicate missing\n        continue\n    \n    # Warn about truncation\n    if len(text) > 512:\n        text = text[:512]\n    \n    try:\n        result = sentiment_pipeline(text)[0]\n        score = result['score']\n        \n        # Convert to -1 to +1 scale\n        if result['label'].upper() == 'POSITIVE':\n            sentiment = score\n        elif result['label'].upper() == 'NEGATIVE':\n            sentiment = -score\n        else:  # NEUTRAL\n            sentiment = 0\n        \n        sentiments.append(sentiment)\n        \n    except torch.cuda.OutOfMemoryError as e:\n        # GPU OOM - clear cache and use None\n        torch.cuda.empty_cache()\n        errors.append(f\"Row {idx}: GPU OOM\")\n        sentiments.append(None)\n    except Exception as e:\n        errors.append(f\"Row {idx}: {type(e).__name__}: {str(e)[:50]}\")\n        sentiments.append(None)\n    \n    # Progress indicator\n    if (idx + 1) % 50 == 0:\n        print(f\"  Processed {idx + 1}/{len(df_raw)} posts...\")\n\n# Report errors if any\nif errors:\n    print(f\"\\nâš ï¸ {len(errors)} inference errors:\")\n    for err in errors[:5]:\n        print(f\"  {err}\")\n    if len(errors) > 5:\n        print(f\"  ... and {len(errors) - 5} more\")\n\n# Add sentiments to dataframe, replacing None with 0 for calculations\ndf_raw['Sentiment_Score'] = sentiments\ndf_raw['Sentiment_Score'] = df_raw['Sentiment_Score'].fillna(0)\n\n# Calculate stats excluding None values\nvalid_sentiments = [s for s in sentiments if s is not None]\nif valid_sentiments:\n    print(f\"\\nâœ“ Sentiment analysis complete\")\n    print(f\"  Valid scores: {len(valid_sentiments)}/{len(sentiments)}\")\n    print(f\"  Mean sentiment: {np.mean(valid_sentiments):.3f}\")\n    print(f\"  Std deviation: {np.std(valid_sentiments):.3f}\")\n    print(f\"  Min: {np.min(valid_sentiments):.3f}, Max: {np.max(valid_sentiments):.3f}\")\nelse:\n    print(\"âš ï¸ No valid sentiment scores calculated\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: PREPARE DATA FOR TRAINING\n",
    "print(\"Preparing training data based on human feedback...\")\n",
    "\n",
    "# Map feedback to labels\n",
    "# Good = 1 (positive/relevant), Bad = 0 (negative/irrelevant), Skip = exclude\n",
    "def map_feedback(feedback):\n",
    "    if feedback == 'Good':\n",
    "        return 1\n",
    "    elif feedback == 'Bad':\n",
    "        return 0\n",
    "    else:\n",
    "        return -1  # Skip or empty\n",
    "\n",
    "df_raw['Feedback_Label'] = df_raw['Feedback'].apply(map_feedback)\n",
    "\n",
    "# Separate labelled and unlabelled data\n",
    "df_labelled = df_raw[df_raw['Feedback_Label'] != -1].copy()\n",
    "df_unlabelled = df_raw[df_raw['Feedback_Label'] == -1].copy()\n",
    "\n",
    "good_count = len(df_labelled[df_labelled['Feedback_Label'] == 1])\n",
    "bad_count = len(df_labelled[df_labelled['Feedback_Label'] == 0])\n",
    "\n",
    "print(f\"\\nFeedback statistics:\")\n",
    "print(f\"  Good posts: {good_count}\")\n",
    "print(f\"  Bad posts: {bad_count}\")\n",
    "print(f\"  Unlabelled/Skip: {len(df_unlabelled)}\")\n",
    "print(f\"  Total labelled: {len(df_labelled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 8: TRAIN/TEST/VALIDATION SPLIT\n# CRITICAL FIX 1.9: Don't assign same data to all splits when < 10 rows\nprint(\"Splitting data into train/validation/test sets...\")\n\n# Flag to track if we have valid splits\nhas_valid_splits = False\n\nif len(df_labelled) >= 30:\n    # Enough data for proper stratified split\n    try:\n        train_val_data, test_data = train_test_split(\n            df_labelled,\n            test_size=0.2,\n            random_state=42,\n            stratify=df_labelled['Feedback_Label'] if len(df_labelled['Feedback_Label'].unique()) > 1 else None\n        )\n        \n        train_data, val_data = train_test_split(\n            train_val_data,\n            test_size=0.125,\n            random_state=42\n        )\n        \n        has_valid_splits = True\n        print(f\"\\nâœ“ Data split complete:\")\n        print(f\"  Training set: {len(train_data)} posts ({len(train_data)/len(df_labelled)*100:.1f}%)\")\n        print(f\"  Validation set: {len(val_data)} posts ({len(val_data)/len(df_labelled)*100:.1f}%)\")\n        print(f\"  Test set (holdout): {len(test_data)} posts ({len(test_data)/len(df_labelled)*100:.1f}%)\")\n        \n    except Exception as e:\n        print(f\"âš ï¸ Could not stratify split: {e}\")\n        print(\"Using simple random split...\")\n        train_data = df_labelled.sample(frac=0.7, random_state=42)\n        remaining = df_labelled.drop(train_data.index)\n        val_data = remaining.sample(frac=0.5, random_state=42)\n        test_data = remaining.drop(val_data.index)\n        has_valid_splits = True\n\nelif len(df_labelled) >= 10:\n    # Limited data - warn about unreliable metrics\n    print(f\"âš ï¸ Limited labelled data ({len(df_labelled)} rows)\")\n    print(\"   Metrics will be calculated but may not be statistically reliable\")\n    \n    train_data = df_labelled.sample(frac=0.6, random_state=42)\n    remaining = df_labelled.drop(train_data.index)\n    val_data = remaining.sample(frac=0.5, random_state=42)\n    test_data = remaining.drop(val_data.index)\n    has_valid_splits = True\n    \n    print(f\"  Training set: {len(train_data)} posts\")\n    print(f\"  Validation set: {len(val_data)} posts\")\n    print(f\"  Test set: {len(test_data)} posts\")\n\nelse:\n    # CRITICAL FIX: Create EMPTY dataframes instead of assigning same data\n    print(f\"âš ï¸ Insufficient labelled data ({len(df_labelled)} < 10)\")\n    print(\"   Cannot calculate reliable model metrics\")\n    print(\"   Collect more feedback via Dashboard to enable model evaluation\")\n    \n    train_data = pd.DataFrame()\n    val_data = pd.DataFrame()\n    test_data = pd.DataFrame()\n    has_valid_splits = False\n\nprint(f\"\\nValid splits available: {has_valid_splits}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 9: MODEL EVALUATION\n# CRITICAL FIX 1.10: Don't use hardcoded 0.85 when no validation data\nprint(\"Evaluating model performance...\")\n\nif has_valid_splits and len(val_data) > 0:\n    # Calculate metrics only with valid validation data\n    y_true = val_data['Feedback_Label'].values\n    \n    # Check for NaN in sentiment scores\n    sentiment_scores = val_data['Sentiment_Score'].values\n    if np.isnan(sentiment_scores).any():\n        print(f\"âš ï¸ {np.isnan(sentiment_scores).sum()} NaN values in Sentiment_Score\")\n    \n    y_pred = (sentiment_scores > 0.5).astype(int)\n    \n    # Calculate metrics\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, zero_division=0)\n    recall = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    \n    model_accuracy = accuracy * 100\n    metrics_available = True\n    \n    print(f\"\"\"\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\nâ•‘     MODEL EVALUATION RESULTS       â•‘\nâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\nâ•‘ Accuracy:  {accuracy:>6.2%}                 â•‘\nâ•‘ Precision: {precision:>6.2%}                 â•‘\nâ•‘ Recall:    {recall:>6.2%}                 â•‘\nâ•‘ F1-Score:  {f1:>6.2%}                 â•‘\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n    \"\"\")\n    \n    # Alert if accuracy below threshold\n    threshold = CONFIG.get('MODEL_ACCURACY_THRESHOLD', 0.85) * 100\n    \n    if model_accuracy < threshold:\n        print(f\"âš ï¸ WARNING: Model accuracy ({model_accuracy:.1f}%) below threshold ({threshold}%)\")\n        print(\"   Consider collecting more feedback data for retraining.\")\n    else:\n        print(f\"âœ“ Model accuracy ({model_accuracy:.1f}%) above threshold ({threshold}%)\")\n        \nelse:\n    # CRITICAL FIX: Don't fake accuracy - indicate it's unknown\n    print(\"âš ï¸ No validation data available - metrics cannot be calculated\")\n    print(\"   Collect at least 10 feedback samples to enable model evaluation\")\n    \n    accuracy = None\n    precision = None\n    recall = None\n    f1 = None\n    model_accuracy = None  # NOT 0.85!\n    metrics_available = False\n    \n    print(\"\\n   Using placeholder values for downstream processing\")\n    print(\"   These do NOT represent actual model performance\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 10: LOAD SOURCE WEIGHTS AND SOURCE CONFIG\nprint(\"Loading source weights...\")\n\ntry:\n    weights_sheet = spreadsheet.worksheet(\"Source Weights\")\n    weights_data = weights_sheet.get_all_records()\n    \n    source_weights = {}\n    for row in weights_data:\n        platform = row.get(\"Source\", \"\").strip()\n        weight = float(row.get(\"Weight_Score\", 5))\n        if platform:\n            source_weights[platform] = weight\n    \n    print(f\"âœ“ Loaded platform weights: {source_weights}\")\n    \nexcept Exception as e:\n    print(f\"âš ï¸ Could not load Source Weights sheet: {e}\")\n    print(\"Using default weights...\")\n    source_weights = {\n        \"TikTok\": 10,\n        \"Instagram\": 9,\n        \"YouTube\": 8,\n        \"Facebook\": 7\n    }\n\n# Load source-specific importance ratings from Source Config\nprint(\"\\nLoading source-specific importance ratings...\")\n\nsource_importance = {}  # (source_name, platform) -> importance (1-5)\n\ntry:\n    source_config_sheet = spreadsheet.worksheet(\"Source Config\")\n    source_config_data = source_config_sheet.get_all_records()\n    \n    for row in source_config_data:\n        source_name = row.get(\"Source_Name\", \"\").strip()\n        platform = row.get(\"Platform\", \"\").strip()\n        importance = int(row.get(\"Importance_Score\", 3))\n        \n        if source_name and platform:\n            key = (source_name, platform)\n            source_importance[key] = importance\n    \n    print(f\"âœ“ Loaded {len(source_importance)} source-specific ratings\")\n    \n    # Show sample ratings\n    if source_importance:\n        print(\"\\nSample source ratings:\")\n        for i, ((name, platform), score) in enumerate(list(source_importance.items())[:5]):\n            print(f\"  {name} ({platform}): {'â˜…' * score}{'â˜†' * (5-score)}\")\n        if len(source_importance) > 5:\n            print(f\"  ... and {len(source_importance) - 5} more\")\n    \nexcept Exception as e:\n    print(f\"âš ï¸ Could not load Source Config sheet: {e}\")\n    print(\"Source-specific ratings will not be used\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 11: CALCULATE WEIGHTED SCORES\nprint(\"Calculating weighted popularity scores...\")\n\ndef calculate_weighted_score(row):\n    \"\"\"\n    Calculate weighted score combining:\n    1. Platform weight (1-10, from Source Weights sheet)\n    2. Source-specific importance (1-5, from Source Config sheet)\n    3. Sentiment score\n    \n    Formula: sentiment * (platform_weight/10 + source_importance/5) / 2\n    \"\"\"\n    platform = row.get('Platform', '')\n    account_name = row.get('Account_Name', '')\n    sentiment = row.get('Sentiment_Score', 0)\n    \n    # Platform weight (1-10, normalized to 0-1)\n    platform_weight = source_weights.get(platform, 5) / 10\n    \n    # Source-specific importance (1-5, normalized to 0-1)\n    source_key = (account_name, platform)\n    source_score = source_importance.get(source_key, 3) / 5\n    \n    # Combined weight: average of platform and source importance\n    combined_weight = (platform_weight + source_score) / 2\n    \n    return sentiment * combined_weight\n\ndf_raw['Weighted_Score'] = df_raw.apply(calculate_weighted_score, axis=1)\n\nprint(f\"âœ“ Calculated weighted scores\")\nprint(f\"  Formula: sentiment Ã— (platform_weight/10 + source_importance/5) / 2\")\nprint(f\"  Mean weighted score: {df_raw['Weighted_Score'].mean():.3f}\")\n\n# Show weight breakdown for top posts\nif len(df_raw) > 0:\n    print(\"\\nSample score breakdown (top 3 posts):\")\n    top_posts = df_raw.nlargest(3, 'Weighted_Score')\n    for idx, row in top_posts.iterrows():\n        platform = row.get('Platform', '')\n        account = row.get('Account_Name', '')\n        sentiment = row.get('Sentiment_Score', 0)\n        pw = source_weights.get(platform, 5) / 10\n        si = source_importance.get((account, platform), 3) / 5\n        weighted = row.get('Weighted_Score', 0)\n        print(f\"  {account[:20]:<20} | Sentiment: {sentiment:>6.3f} | Platform: {pw:.1f} | Source: {si:.1f} | Final: {weighted:>6.3f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 12: AGGREGATE BY CELEBRITY\nprint(\"Aggregating results by celebrity...\")\n\n# CRITICAL FIX 3: Load previous day's results for real trend calculation\nprevious_scores = {}\nprevious_source_breakdown = {}  # Fix 9: Track previous source breakdown\ntry:\n    print(\"Loading previous results for trend comparison...\")\n    prev_results_sheet = spreadsheet.worksheet(\"Results\")\n    prev_results_data = prev_results_sheet.get_all_records()\n    \n    if prev_results_data:\n        for row in prev_results_data:\n            celebrity = row.get('Celebrity', '')\n            prev_score = row.get('Weighted_Popularity_Score', 0)\n            prev_breakdown = row.get('Source_Breakdown', '{}')\n            if celebrity and prev_score:\n                try:\n                    previous_scores[celebrity] = float(prev_score)\n                    previous_source_breakdown[celebrity] = json.loads(prev_breakdown) if prev_breakdown else {}\n                except (ValueError, TypeError, json.JSONDecodeError):\n                    pass\n        print(f\"âœ“ Loaded {len(previous_scores)} previous celebrity scores\")\n    else:\n        print(\"âš ï¸ No previous results found - all trends will be 'Stable'\")\nexcept Exception as e:\n    print(f\"âš ï¸ Could not load previous results: {e}\")\n    print(\"   All trends will be shown as 'Stable'\")\n\ndef calculate_trend(celebrity, current_score):\n    \"\"\"\n    FIX 8: Calculate trend direction with velocity levels.\n    Thresholds:\n      - ğŸš€ Fast Rising: delta > 0.15\n      - â†‘ Rising: delta > 0.05\n      - â†’ Stable: -0.05 <= delta <= 0.05\n      - â†“ Falling: delta < -0.05\n      - ğŸ“‰ Fast Falling: delta < -0.15\n    \"\"\"\n    if celebrity not in previous_scores:\n        return 'â†’ Stable'  # No previous data\n    \n    prev_score = previous_scores[celebrity]\n    delta = current_score - prev_score\n    \n    if delta > 0.15:\n        return 'ğŸš€ Fast Rising'\n    elif delta > 0.05:\n        return 'â†‘ Rising'\n    elif delta < -0.15:\n        return 'ğŸ“‰ Fast Falling'\n    elif delta < -0.05:\n        return 'â†“ Falling'\n    else:\n        return 'â†’ Stable'\n\ndef calculate_risk_flag(celebrity, current_score):\n    \"\"\"\n    Flag if sentiment dropped >20% compared to previous.\n    \"\"\"\n    if celebrity not in previous_scores:\n        return 'No'\n    \n    prev_score = previous_scores[celebrity]\n    if prev_score == 0:\n        return 'No'\n    \n    pct_change = (current_score - prev_score) / abs(prev_score)\n    return 'Yes' if pct_change < -0.20 else 'No'\n\ndef calculate_endorsement_ready(row):\n    \"\"\"\n    Determine if celebrity is ready for endorsement based on:\n    - Confidence >= 70%\n    - Sentiment StdDev <= 0.25 (low volatility)\n    - Good Records Ratio >= 60%\n    \"\"\"\n    confidence = row.get('Confidence_Score', 0)\n    stddev = row.get('Sentiment_StdDev', 1)\n    good_ratio = row.get('Good_Records_Ratio', 0)\n    \n    confidence_threshold = CONFIG.get('CONFIDENCE_THRESHOLD', 0.70) * 100\n    stddev_max = CONFIG.get('SENTIMENT_STDDEV_MAX', 0.25)\n    \n    if confidence >= confidence_threshold and stddev <= stddev_max and good_ratio >= 60:\n        return 'Yes'\n    return 'No'\n\ndef calculate_top_contributing_source(celebrity, current_breakdown):\n    \"\"\"\n    FIX 9: Identify which platform contributed most to positive change.\n    Compares current vs previous source breakdown.\n    \"\"\"\n    prev_breakdown = previous_source_breakdown.get(celebrity, {})\n    \n    if not current_breakdown:\n        return ''\n    \n    # Calculate delta per platform\n    deltas = {}\n    for platform, current_val in current_breakdown.items():\n        prev_val = prev_breakdown.get(platform, 0)\n        delta = float(current_val) - float(prev_val)\n        deltas[platform] = delta\n    \n    # Find platform with highest positive delta\n    if deltas:\n        top_platform = max(deltas, key=deltas.get)\n        top_delta = deltas[top_platform]\n        if top_delta > 0:\n            return f\"{top_platform} (+{top_delta:.2f})\"\n        elif top_delta < 0:\n            # If all negative, show least negative\n            return f\"{top_platform} ({top_delta:.2f})\"\n        else:\n            return top_platform\n    return ''\n\ndef calculate_score_change_breakdown(celebrity, current_breakdown):\n    \"\"\"\n    FIX 9: Calculate score change per platform (JSON).\n    Shows delta by platform for detailed analysis.\n    \"\"\"\n    prev_breakdown = previous_source_breakdown.get(celebrity, {})\n    \n    if not current_breakdown:\n        return '{}'\n    \n    # Calculate delta per platform\n    change_breakdown = {}\n    all_platforms = set(list(current_breakdown.keys()) + list(prev_breakdown.keys()))\n    \n    for platform in all_platforms:\n        current_val = float(current_breakdown.get(platform, 0))\n        prev_val = float(prev_breakdown.get(platform, 0))\n        delta = current_val - prev_val\n        change_breakdown[platform] = round(delta, 3)\n    \n    return json.dumps(change_breakdown)\n\nif len(df_raw) > 0:\n    # Group by celebrity and calculate statistics\n    results = df_raw.groupby('Celebrity').agg({\n        'Sentiment_Score': ['mean', 'std', 'count'],\n        'Weighted_Score': 'mean',\n        'Platform': lambda x: x.value_counts().index[0] if len(x.dropna()) > 0 else 'Unknown'\n    }).round(4)\n    \n    # Flatten column names\n    results.columns = ['Avg_Sentiment_Raw', 'Sentiment_StdDev', 'Total_Posts_Analyzed',\n                       'Weighted_Popularity_Score', 'Top_Source']\n    \n    # Fill NaN std with 0\n    results['Sentiment_StdDev'] = results['Sentiment_StdDev'].fillna(0)\n    \n    # Add confidence - use actual accuracy or \"N/A\" if not available\n    display_accuracy = model_accuracy if model_accuracy is not None else 0\n    results['Confidence_Score'] = display_accuracy\n    \n    # Calculate real confidence intervals based on std dev\n    def calc_score_range(row):\n        score = row['Weighted_Popularity_Score']\n        std = row['Sentiment_StdDev']\n        n = max(1, row['Total_Posts_Analyzed'])\n        margin = 1.96 * std / np.sqrt(n)  # 95% CI\n        lower = max(-1, score - margin)\n        upper = min(1, score + margin)\n        return f\"{lower:.2f} - {upper:.2f}\"\n    \n    results['Score_Range'] = results.apply(calc_score_range, axis=1)\n    results['Model_Accuracy'] = display_accuracy\n    \n    # CRITICAL FIX 3 + FIX 8: Calculate real trend direction with velocity\n    results = results.reset_index()  # Make Celebrity a column\n    results['Trend_Direction'] = results.apply(\n        lambda row: calculate_trend(row['Celebrity'], row['Weighted_Popularity_Score']), axis=1\n    )\n    \n    # Calculate source breakdown for each celebrity (dict for internal use)\n    source_breakdown = {}\n    source_breakdown_raw = {}  # Keep raw dict for Fix 9\n    for celeb in results['Celebrity']:\n        celeb_data = df_raw[df_raw['Celebrity'] == celeb]\n        breakdown = celeb_data.groupby('Platform')['Sentiment_Score'].mean().to_dict()\n        # Handle NaN values in breakdown\n        breakdown_clean = {k: round(v, 2) for k, v in breakdown.items() if pd.notna(v)}\n        source_breakdown_raw[celeb] = breakdown_clean\n        source_breakdown[celeb] = json.dumps(breakdown_clean)\n    \n    results['Source_Breakdown'] = results['Celebrity'].map(source_breakdown)\n    \n    # FIX 9: Calculate Top Contributing Source\n    results['Top_Contributing_Source'] = results['Celebrity'].apply(\n        lambda celeb: calculate_top_contributing_source(celeb, source_breakdown_raw.get(celeb, {}))\n    )\n    \n    # FIX 9: Calculate Score Change Breakdown\n    results['Score_Change_Breakdown'] = results['Celebrity'].apply(\n        lambda celeb: calculate_score_change_breakdown(celeb, source_breakdown_raw.get(celeb, {}))\n    )\n    \n    # Calculate good records ratio (only for labelled data)\n    good_ratio = {}\n    for celeb in results['Celebrity']:\n        celeb_labelled = df_labelled[df_labelled['Celebrity'] == celeb]\n        if len(celeb_labelled) > 0:\n            ratio = (celeb_labelled['Feedback_Label'] == 1).mean() * 100\n            good_ratio[celeb] = round(ratio, 1)\n        else:\n            good_ratio[celeb] = 0\n    \n    results['Good_Records_Ratio'] = results['Celebrity'].map(good_ratio)\n    \n    # Calculate Risk Flag (sentiment drop > 20%)\n    results['Risk_Flag'] = results.apply(\n        lambda row: calculate_risk_flag(row['Celebrity'], row['Weighted_Popularity_Score']), axis=1\n    )\n    \n    # Calculate Endorsement Ready status\n    results['Endorsement_Ready'] = results.apply(calculate_endorsement_ready, axis=1)\n    \n    # Sort by weighted score and add rank\n    results = results.sort_values('Weighted_Popularity_Score', ascending=False)\n    results = results.reset_index(drop=True)\n    results.insert(0, 'Rank', range(1, len(results) + 1))\n    \n    print(f\"\\nâœ“ Aggregation complete\")\n    print(f\"\\nRankings with Trends (Fix 8: Velocity Indicators):\")\n    print(results[['Rank', 'Celebrity', 'Weighted_Popularity_Score', 'Trend_Direction', 'Top_Contributing_Source']].to_string(index=False))\n    \n    # Show trend changes with velocity\n    fast_rising = results[results['Trend_Direction'].str.contains('Fast Rising')]\n    rising = results[results['Trend_Direction'].str.contains('Rising') & ~results['Trend_Direction'].str.contains('Fast')]\n    fast_falling = results[results['Trend_Direction'].str.contains('Fast Falling')]\n    falling = results[results['Trend_Direction'].str.contains('Falling') & ~results['Trend_Direction'].str.contains('Fast')]\n    risky = results[results['Risk_Flag'] == 'Yes']\n    \n    if len(fast_rising) > 0:\n        print(f\"\\nğŸš€ Fast Rising celebrities: {', '.join(fast_rising['Celebrity'].tolist())}\")\n    if len(rising) > 0:\n        print(f\"\\nğŸ“ˆ Rising celebrities: {', '.join(rising['Celebrity'].tolist())}\")\n    if len(fast_falling) > 0:\n        print(f\"\\nğŸ“‰ Fast Falling celebrities: {', '.join(fast_falling['Celebrity'].tolist())}\")\n    if len(falling) > 0:\n        print(f\"\\nâ†“ Falling celebrities: {', '.join(falling['Celebrity'].tolist())}\")\n    if len(risky) > 0:\n        print(f\"\\nâš ï¸ Risk alerts (>20% drop): {', '.join(risky['Celebrity'].tolist())}\")\n    \n    # Check for missing configured celebrities\n    configured_celebs = set(CONFIG.get('CELEBRITIES_TO_TRACK', []))\n    found_celebs = set(results['Celebrity'])\n    missing_celebs = configured_celebs - found_celebs\n    if missing_celebs:\n        print(f\"\\nâš ï¸ WARNING: No data found for celebrities: {missing_celebs}\")\n    \nelse:\n    print(\"âš ï¸ No data to aggregate\")\n    results = pd.DataFrame()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 13: PREPARE RESULTS FOR SHEET\nprint(\"Preparing results for output...\")\n\nif len(results) > 0:\n    # Select and order columns for Results sheet\n    # FIX 9: Added Top_Contributing_Source and Score_Change_Breakdown columns\n    output_columns = [\n        'Rank', 'Celebrity', 'Avg_Sentiment_Raw', 'Total_Posts_Analyzed',\n        'Sentiment_StdDev', 'Weighted_Popularity_Score', 'Confidence_Score',\n        'Score_Range', 'Model_Accuracy', 'Trend_Direction', 'Source_Breakdown',\n        'Top_Source', 'Good_Records_Ratio', 'Risk_Flag', 'Endorsement_Ready',\n        'Top_Contributing_Source', 'Score_Change_Breakdown'  # Fix 9: New columns\n    ]\n    \n    # Ensure all columns exist\n    for col in output_columns:\n        if col not in results.columns:\n            results[col] = ''\n    \n    results_output = results[output_columns].copy()\n    \n    # Add timestamp and notes\n    results_output['Last_Updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    results_output['Analysis_Notes'] = results_output.apply(\n        lambda row: f\"Score: {row['Weighted_Popularity_Score']:.2f} | Posts: {row['Total_Posts_Analyzed']} | Trend: {row['Trend_Direction']}\",\n        axis=1\n    )\n    \n    print(f\"âœ“ Prepared {len(results_output)} rows for output\")\n    print(f\"\\nOutput columns: {list(results_output.columns)}\")\n    \n    # Show endorsement summary\n    ready_count = len(results_output[results_output['Endorsement_Ready'] == 'Yes'])\n    print(f\"\\nEndorsement Summary:\")\n    print(f\"  Ready for endorsement: {ready_count}\")\n    print(f\"  Not ready: {len(results_output) - ready_count}\")\n    \n    # FIX 9: Show source attribution summary\n    print(f\"\\nSource Attribution Summary (Fix 9):\")\n    for idx, row in results_output.head(5).iterrows():\n        celeb = row['Celebrity']\n        top_src = row['Top_Contributing_Source']\n        if top_src:\n            print(f\"  {celeb}: {top_src}\")\n    \nelse:\n    results_output = pd.DataFrame()\n    print(\"âš ï¸ No results to output\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 14: WRITE RESULTS TO SHEET\n# CRITICAL FIX 1.12: Add retry logic for sheet writes\nimport time\n\nMAX_RETRIES = 3\nRETRY_DELAY = 5  # seconds\n\nprint(\"Writing results to Results sheet...\")\n\nif len(results_output) > 0:\n    for attempt in range(MAX_RETRIES):\n        try:\n            results_sheet = spreadsheet.worksheet(\"Results\")\n            \n            # Clear existing data\n            results_sheet.clear()\n            time.sleep(1)  # Brief pause after clear\n            \n            # Prepare all data as list of lists\n            header = results_output.columns.tolist()\n            data_rows = []\n            for idx, row in results_output.iterrows():\n                row_values = [str(v) if pd.notna(v) else '' for v in row.tolist()]\n                data_rows.append(row_values)\n            \n            # Write header\n            results_sheet.append_row(header)\n            \n            # Batch write data rows (more efficient than row-by-row)\n            if data_rows:\n                results_sheet.append_rows(data_rows)\n            \n            # Verify write\n            written_count = results_sheet.row_count - 1  # Subtract header\n            print(f\"âœ“ Wrote {len(results_output)} rows to Results sheet\")\n            break  # Success, exit retry loop\n            \n        except gspread.exceptions.APIError as e:\n            if 'quotaExceeded' in str(e):\n                print(f\"ERROR: Google Sheets API quota exceeded\")\n                raise\n            elif attempt < MAX_RETRIES - 1:\n                print(f\"âš ï¸ API error (attempt {attempt + 1}/{MAX_RETRIES}): {e}\")\n                print(f\"   Retrying in {RETRY_DELAY} seconds...\")\n                time.sleep(RETRY_DELAY)\n            else:\n                print(f\"âœ— Failed after {MAX_RETRIES} attempts: {e}\")\n                raise\n        except Exception as e:\n            if attempt < MAX_RETRIES - 1:\n                print(f\"âš ï¸ Error (attempt {attempt + 1}/{MAX_RETRIES}): {e}\")\n                time.sleep(RETRY_DELAY)\n            else:\n                print(f\"âœ— Error writing to Results sheet: {e}\")\n                raise\nelse:\n    print(\"âš ï¸ No results to write\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 15: RECORD FEEDBACK TO FEEDBACK HISTORY\nprint(\"Recording feedback to Feedback History sheet...\")\n\nif len(df_labelled) > 0:\n    try:\n        feedback_sheet = spreadsheet.worksheet(\"Feedback History\")\n        \n        # Get existing post IDs to avoid duplicates\n        existing_data = feedback_sheet.get_all_records()\n        existing_ids = set(row.get('Post_ID', '') for row in existing_data)\n        \n        new_entries = 0\n        for idx, row in df_labelled.iterrows():\n            post_id = f\"raw_data_row_{idx}\"\n            \n            # Skip if already recorded\n            if post_id in existing_ids:\n                continue\n            \n            feedback_entry = [\n                post_id,\n                str(row.get('Post_Content', ''))[:500],  # Truncate long content\n                round(row.get('Sentiment_Score', 0), 4),\n                int(row.get('Feedback_Label', 0)),\n                str(row.get('Feedback_Notes', '')),\n                datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n                1  # Feedback round\n            ]\n            feedback_sheet.append_row(feedback_entry)\n            new_entries += 1\n        \n        print(f\"âœ“ Recorded {new_entries} new feedback entries\")\n        print(f\"  (Skipped {len(df_labelled) - new_entries} existing entries)\")\n        \n    except Exception as e:\n        print(f\"âœ— Error writing to Feedback History: {e}\")\nelse:\n    print(\"âš ï¸ No labelled data to record\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 16: WRITE MODEL METRICS TO AUDIT SHEET\nprint(\"Writing model metrics to audit trail...\")\n\ntry:\n    metrics_sheet = spreadsheet.worksheet(\"Model Metrics\")\n    \n    # Prepare metrics row - handle None values\n    good_count = len(df_labelled[df_labelled['Feedback_Label'] == 1]) if len(df_labelled) > 0 else 0\n    bad_count = len(df_labelled[df_labelled['Feedback_Label'] == 0]) if len(df_labelled) > 0 else 0\n    skip_count = len(df_raw) - len(df_labelled)\n    \n    threshold = CONFIG.get('MODEL_ACCURACY_THRESHOLD', 0.85) * 100\n    \n    # Handle None accuracy (when no validation data)\n    acc_str = f\"{accuracy*100:.1f}%\" if accuracy is not None else \"N/A\"\n    prec_str = f\"{precision*100:.1f}%\" if precision is not None else \"N/A\"\n    rec_str = f\"{recall*100:.1f}%\" if recall is not None else \"N/A\"\n    f1_str = f\"{f1*100:.1f}%\" if f1 is not None else \"N/A\"\n    \n    # Determine status based on metrics availability\n    if model_accuracy is not None:\n        model_status = \"PASSED\" if model_accuracy >= threshold else \"FAILED\"\n        pipeline_status = \"SUCCESS\" if model_accuracy >= threshold else \"WARNING\"\n    else:\n        model_status = \"NO_DATA\"\n        pipeline_status = \"INCOMPLETE\"\n    \n    metrics_row = [\n        datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n        len(df_raw),\n        good_count,\n        bad_count,\n        skip_count,\n        acc_str,\n        prec_str,\n        rec_str,\n        f1_str,\n        model_status,\n        len(results) if len(results) > 0 else 0,\n        \", \".join(results['Celebrity'].tolist()) if len(results) > 0 else \"\",\n        pipeline_status,\n        \"\"  # Error log (empty if no errors)\n    ]\n    \n    metrics_sheet.append_row(metrics_row)\n    \n    print(f\"âœ“ Wrote metrics to audit trail\")\n    print(f\"  Run ID: {metrics_row[1]}\")\n    print(f\"  Status: {pipeline_status}\")\n    \nexcept Exception as e:\n    print(f\"âœ— Error writing to Model Metrics: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 17: UPDATE RAW DATA WITH SENTIMENT SCORES\n# CRITICAL FIX 1.13: Batch updates instead of row-by-row\nprint(\"Updating Raw Data sheet with sentiment scores...\")\n\nif len(df_raw) > 0:\n    try:\n        raw_sheet = spreadsheet.worksheet(\"Raw Data\")\n        \n        # Validate header exists\n        header = raw_sheet.row_values(1)\n        if not header:\n            raise ValueError(\"Raw Data sheet has no header\")\n        \n        # Find column indices\n        if 'Sentiment_Score' not in header:\n            print(\"âš ï¸ 'Sentiment_Score' column not found - skipping update\")\n        else:\n            sentiment_col = header.index('Sentiment_Score') + 1\n            processing_col = header.index('Processing_Date') + 1 if 'Processing_Date' in header else None\n            \n            processing_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            \n            # Prepare batch update data\n            sentiment_updates = []\n            processing_updates = []\n            \n            for idx, row in df_raw.iterrows():\n                row_num = idx + 2  # +2 for 0-indexing and header row\n                sentiment_score = round(float(row.get('Sentiment_Score', 0)), 4)\n                \n                # Collect updates for batch\n                sentiment_updates.append({\n                    'range': f\"K{row_num}\",  # Column K = Sentiment_Score\n                    'values': [[sentiment_score]]\n                })\n                \n                if processing_col:\n                    processing_updates.append({\n                        'range': f\"L{row_num}\",  # Column L = Processing_Date\n                        'values': [[processing_date]]\n                    })\n            \n            # Batch update sentiment scores\n            if sentiment_updates:\n                # Use batch_update for efficiency\n                spreadsheet.values_batch_update({\n                    'valueInputOption': 'RAW',\n                    'data': sentiment_updates\n                })\n                \n            # Batch update processing dates\n            if processing_updates:\n                spreadsheet.values_batch_update({\n                    'valueInputOption': 'RAW',\n                    'data': processing_updates\n                })\n            \n            print(f\"âœ“ Updated {len(df_raw)} rows with sentiment scores (batch)\")\n        \n    except Exception as e:\n        print(f\"âœ— Error updating Raw Data: {e}\")\n        print(\"  (Sentiment scores calculated but not saved to sheet)\")\nelse:\n    print(\"âš ï¸ No data to update\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 18: SUMMARY & ALERTS\nthreshold = CONFIG.get('MODEL_ACCURACY_THRESHOLD', 0.85) * 100\ngood_count = len(df_labelled[df_labelled['Feedback_Label'] == 1]) if len(df_labelled) > 0 else 0\nbad_count = len(df_labelled[df_labelled['Feedback_Label'] == 0]) if len(df_labelled) > 0 else 0\ntop_celebrity = results.iloc[0]['Celebrity'] if len(results) > 0 else 'N/A'\n\n# Handle None accuracy for display\nacc_display = f\"{model_accuracy:.1f}%\" if model_accuracy is not None else \"N/A\"\n\nprint(f\"\"\"\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\nâ•‘           PIPELINE COMPLETE âœ“                     â•‘\nâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\nâ•‘ Total posts collected:     {len(df_raw):>20} â•‘\nâ•‘ Posts with feedback:       {len(df_labelled):>20} â•‘\nâ•‘ Good posts:                {good_count:>20} â•‘\nâ•‘ Bad posts:                 {bad_count:>20} â•‘\nâ•‘ Model accuracy:            {acc_display:>20} â•‘\nâ•‘ Celebrities ranked:        {len(results):>20} â•‘\nâ•‘ Top celebrity:             {top_celebrity:>20} â•‘\nâ•‘ Timestamp:                 {datetime.now().strftime('%Y-%m-%d %H:%M:%S'):>20} â•‘\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\"\"\")\n\n# Alerts\nif model_accuracy is not None:\n    if model_accuracy < threshold:\n        print(f\"âš ï¸  ACTION REQUIRED: Model accuracy below {threshold}%\")\n        print(f\"   Current: {model_accuracy:.1f}%\")\n        print(f\"   Recommendation: Collect more feedback data and retrain\")\n    else:\n        print(f\"âœ“ Model ready for production (accuracy: {model_accuracy:.1f}%)\")\nelse:\n    print(\"âš ï¸  NOTE: Model accuracy not available (need 10+ feedback samples)\")\n    print(\"   Use the Dashboard to provide feedback on posts\")\n\nif len(df_labelled) < CONFIG.get('TRAINING_DATA_MIN', 200):\n    print(f\"\\nâš ï¸  NOTE: Training data ({len(df_labelled)}) below minimum ({CONFIG.get('TRAINING_DATA_MIN', 200)})\")\n    print(f\"   Continue collecting feedback for better model performance\")\n\nprint(f\"\\nâœ“ Pipeline execution completed successfully!\")\nprint(f\"   Check the 'Results' sheet for celebrity rankings.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}